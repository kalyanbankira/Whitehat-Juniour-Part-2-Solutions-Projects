# -*- coding: utf-8 -*-
"""project 91.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wloC-IsIIJaPMyyaomLXnk5R4HXcgDAI

### Instructions

#### Goal of the Project
This project is designed for you to practice and solve the activities that are based on the concepts covered in the lesson:

  - Support Vector Machines - Regression
  - Hyperparameter Tuning

---

#### Getting Started:

1. Click on this link to open the Colab file for this project.

   https://colab.research.google.com/drive/1SHLHHpzCu0IRIeQM9c5htFT5lryNO9h7

2. Create a duplicate copy of the Colab file as described below.

  - Click on the **File menu**. A new drop-down list will appear.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-0/0_file_menu.png' width=500>

  - Click on the **Save a copy in Drive** option. A duplicate copy will get created. It will open up in the new tab on your web browser.

  <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-0/1_create_colab_duplicate_copy.png' width=500>

3. After creating the duplicate copy of the notebook, please rename it in the **YYYY-MM-DD_StudentName_Project91** format.

4. Now, write your code in the prescribed code cells.

---

#### Problem Statement

In this project, you are going to revise the lesson by creating a dummy dataset and its exploration.

---

### List of Activities

**Activity 1:** Create Dummy Dataset

**Activity 2:**  Data Exploration

---

#### Activity 1: Create Dummy Dataset

In this activity, generate a dummy dataset using the `make_regression()` function of the `sklearn.datasets` module.


**1.** Generate feature array and target array using `make_regression()` function. Use following specifications:
  
  - Number of samples: 30
  - Number of independent features: 10
  - Number of informative features: 2
  - Number of dependent features: 1
  - Random state: 12  

**2.** Print the number of rows and columns for each array.
"""

# Generate dummy dataset using the 'make_regression()' function
# Import the modules
from sklearn.datasets import make_regression

# Create the arrays
reg_feat,reg_target = make_regression(n_samples = 30,n_features = 10,n_informative = 2,n_targets = 1,random_state = 12)

# Print the object-type of the arrays created by the 'make_regression()' function and the number of rows and columns in them.
print(f"Type :{type(reg_feat)}")
print(f"Type :{type(reg_target)}")
print('----'*10)
print(f"Shape :{reg_feat.shape}")
print(f"Shape:{reg_target.shape}")

"""**Hint:** The syntax for the `make_regression()` function is as follows:

   **Syntax:** `make_regression(n_samples, n_features,  n_informative, n_targets,  random_state)`

   Where,

   - `n_samples`- Determines the number of records to be generated in a dataset.

   - `n_features` - Determines the number of features (or independent variables) to be generated in the dataset.

- `n_informative` - Determines the number of features contributing to the prediction of the target variable required to build a regression model.

- `n_targets` - Determines the number of target (or dependent) variables.

- `random_state`- Determines whether the same set of random values to be generated over and over instead of new ones. An integer value of the `random_state` parameter will produce the same results across different function calls. Popular integer random seeds are 0 and 42.

**Q:** How many rows and columns do the output array has?

**A:** Both has 30 rows and feature has 10 columns where as target has no columns.

---

**3.**  Create a Pandas DataFrame with the two output arrays created above.

**Steps:**

1. Create an empty dictionary.
2. Create a `for` loop to iterates from 0 to the total number of columns (features). Inside the `for` loop:

   i. Add $i^\text{th}$ feature to the dictionary using the index `i` and string `"feature " + str(i +1)` such that the keys are `feature 1`, `feature 2` ... `feature n` (n is the number of independent feature columns).

   ii. Add data of column `i` from the features array as values to the $i^\text{th}$ key in the dictionary.
    
3. Add the last key in the dictionary as `"target"` and value as target array.

4. Create DataFrame from the dictionary created above using `pd.DataFrame()` function.
"""

# Creating Pandas DataFrame containing the items from the output arrays.
import pandas as pd
# A dummy dictionary
new_dict ={}
# Create a for loop to add the features (keys) and data in the dictionary


for i in range(reg_feat.shape[1]):
    new_dict["feature" + str(i + 1)] = reg_feat[:,i]
# Add the target key data in the dictionary
new_dict['target'] = reg_target
# Create DataFrame from the dictionary
#new_df = pd.DataFrame(data = new_dict)

new_df = pd.DataFrame(data=new_dict)
new_df.head()

"""**Hint:** A dummy DataFrame is created from the two arrays using a Python dictionary. *(Learned in "Activity 2: Support Vector Regression (SVR)" from "Support Vector Machines - Regression" lesson)*

**After this activity, the DataFrame should be created with the independent feature columns and dependent target column.**

---

####Activity 2: Data Exploration

In this activity, you will explore the data to understand the distribution of data and find the columns correlated with the `target` column.

**1.** Print the descriptive statistics of the data:
"""

# Print the descriptive statistics of the dummy DataFrame
print(new_df.describe())

"""**Q:** Are the feature column values in the same or almost the same range?

**A:** YES

---

**2.**  Check whether the values in each column follow a normal distribution using histogram:
"""

# Create a histogram for all the columns in the above data frame to check whether the values in each column follow the normal distribution.
# Import the modules
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Create the for loop for the histogram
for i in new_df.columns:
    plt.figure(figsize = (8, 4), dpi = 96)
    plt.title(f"Histogram for {i}", fontsize = 15)
    plt.hist(new_df[i], bins = "sturges", edgecolor = "black")
    plt.axvline(np.mean(new_df[i]), color = "red")
    plt.legend()
    plt.show()

"""**Q:** Does the independent feature column values follow normal distribution?

**A:**

---

**3.** Create a heatmap to find the correlation coefficient for the DataFrame:
"""

# Create a heatmap for the correlation coefficient DataFrame.

plt.figure(figsize = (10, 5), dpi = 96)
plt.title(f"Heatmap", fontsize = 15)
sns.heatmap(new_df.corr(), annot = True)
plt.show()

"""**Q:** How many independent features columns are strongly correlated with the dependent `target` column? Why?

**A:** Two

**Q:** Which columns are strongly correlated with the dependent `target` column?

**A:** `feature 10` and `feature 4` are strongly correlated with the dependent `target` column.

---

**4.** Create a regression plot between each strongly correlated columns and the target column to observe the relationship and best-fit regression line:
"""

# Create a regplot between strongly related features and the 'target' column.
# First Regression Plot with the shaded region
plt.figure(figsize = (10, 5), dpi = 96)
plt.title("Regplot ")
sns.regplot(x = "feature10", y = "target", data = new_df, color = "r")
plt.show()
# Second Regression Plot without the shaded region

plt.figure(figsize = (8, 4), dpi = 96)
sns.regplot(x = "feature4", y = "target", data = new_df, color = "g", ci = None)
plt.show()

"""Looking into the heatmap and scatterplot answer the following questions:

**Q:** What type of correlation (positive or negative) exists between the strongly correlated feature columns and target column?

**A:** Positive correlation  exists between the strongly correlated feature columns and target column.

**Q:** What does `ci` stand for?

**A:** `ci` stand for `confidence interval`.

**Q:** Which term is used to determine the percentage of target values predicted by chance (or fluke)?

**A:** `ci` is used to determine the percentage of target values predicted by chance (or fluke).

**After this activity, the columns correlated with the dependent target column `target` should be identified.**

----

### Submitting the Project:

1. After finishing the project, click on the **Share** button on the top right corner of the notebook. A new dialog box will appear.

  <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/2_share_button.png' width=500>

2. In the dialog box, make sure that '**Anyone on the Internet with this link can view**' option is selected and then click on the **Copy link** button.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/3_copy_link.png' width=500>

3. The link of the duplicate copy (named as **YYYY-MM-DD_StudentName_Project91**) of the notebook will get copied.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/4_copy_link_confirmation.png' width=500>

4. Go to your dashboard and click on the **My Projects** option.
   
   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/5_student_dashboard.png' width=800>

  <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/6_my_projects.png' width=800>

5. Click on the **View Project** button for the project you want to submit.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/7_view_project.png' width=800>

6. Click on the **Submit Project Here** button.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/8_submit_project.png' width=800>

7. Paste the link to the project file named as **YYYY-MM-DD_StudentName_Project91** in the URL box and then click on the **Submit** button.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/9_enter_project_url.png' width=800>

---
"""