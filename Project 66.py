# -*- coding: utf-8 -*-
"""Project 66.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vXjpc6YujDcrUC_5tTj1qixQia77iaDT

# Capstone Project 16: Diamond Price Prediction

---

### Context

Diamond is one of the precious stones which are always in huge demand in the investment market. Diamonds are also used in many industrial applications like cutting but it is mostly used as a gemstone. The actual price of a diamond however is determined by a gemologist after examining its various features such as its carat, cut, color, and clarity. Dimensions of a diamond is also a very important parameter to determine its worth. Nearly, 142 million carats of diamonds were produced worldwide in 2019 alone. This makes it very important to come up with some smart technique to estimate its worth.

---

### Problem Statement

A diamond distributor decided to put almost 2000 diamonds for auction. A jewellery company is interested in making a bid to purchase these diamonds in order to expand their business. As a data scientist, your job is to build a prediction model to predict the price of diamonds so that your company knows how much it should bid.

---

#### Getting Started

Follow the steps described below to solve the project:

1. Click on the link provided below to open the Colab file for this project.
   
   https://colab.research.google.com/drive/1YP4OiSvsLVur_2GbEcCnbugflUXrlJP-?usp=sharing

2. Create the duplicate copy of the Colab file. Here are the steps to create the duplicate copy:

    - Click on the **File** menu. A new drop-down list will appear.

      <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/0_file_menu.png' width=500>

    - Click on the **Save a copy in Drive** option. A duplicate copy will get created. It will open up in the new tab on your web browser.

      <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/1_create_colab_duplicate_copy.png' width=500>

     - After creating the duplicate copy of the notebook, please rename it in the **YYYY-MM-DD_StudentName_CapstoneProject16** format.

3. Now, write your code in the prescribed code cells.

---

### Data Description

The **diamonds** dataset contains the prices and other attributes of almost 54,000 diamonds. Following are the attributes:  


|Column|Description|
|---:|:---|
|`carat`|weight of the diamond|
|`cut`|quality of the cut|
|`color`|diamond colour, from J (worst) to D (best)|
|`clarity`|a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))|
|`table`|The width of the diamond's table expressed as a percentage of its average diameter|
|`price`|price in US dollars|
|`x`|length in mm|
|`y`|width in mm|
|`z`|depth in mm|
|`depth`|total depth percentage = $\frac{2z}{x + y}$|

  **Dataset Link:** https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/diamonds.csv

---

### Things To Do

1. Explore the diamond dataset by creating the following plots:
   - Box plots between each categorical feature and the `price`.
   - Scatter plots between the numerical features and the `price`.
   
2. Convert categorical attributes into numerical attributes.

3. Create a correlation heatmap for all the columns.

4. Build a linear regression model by selecting the most relevant features to predict the price of diamonds.

5. Reduce multicollinearity (if exists) by eliminating highly correlated and high VIF features.

5. Evaluate the linear regression model by calculating the parameters such as coefficient of determination, MAE, MSE, RMSE, mean of residuals and by checking for homoscedasticity.

---

#### 1. Import Modules and Load Dataset

Link to the dataset: https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/diamonds.csv
"""

# Import the required modules and load the dataset.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get the information on DataFrame.
data = pd.read_csv("https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/diamonds.csv")
data.head()

# Check if there are any null values. If any column has null values, treat them accordingly
data.isnull().sum()

# Drop 'Unnamed: 0' column as it is of no use
data = data.drop(columns=['Unnamed: 0'], axis=1)

"""---

#### 2. Exploratory Data Analysis

We need to predict the value of `price` variable, using other variables. Thus, `price` is the target or dependent variable and other columns except `price` are the features or the independent variables.

Perform the following tasks:

- Create Box plots between each **categorical** variable and the target variable `price` to sense the distribution of values.

- Create the Scatter plots between each **numerical** variable and the target variable `price`. Determine which variable(s) shows linear relationship with the target variable `price`.

- Create a normal distribution curve for the `price`.
"""

data.head()

# Boxplot for 'cut' vs 'price'
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Boxplot of Cut Vs Price Graph",fontsize = 20)
sns.boxplot(x = data['cut'],y = data['price'])
plt.xlabel("Cut ----> ")
plt.ylabel(" Price------>")
plt.show()

# Boxplot for 'color' vs 'price'
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Boxplot of Color Vs Price Data Graph",fontsize = 20)
sns.boxplot(x = data['color'],y = data['price'])
plt.xlabel("Color ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Boxplot for 'clarity' vs 'price'
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Boxplot of Clarity Vs Price Data Graph",fontsize = 20)
sns.boxplot(x = data['clarity'],y = data['price'])
plt.xlabel("Clarity ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Create scatter plot with 'carat' on X-axis and 'price' on Y-axis
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Scatter plot of Carat Vs Price Data Graph",fontsize = 20)
plt.scatter(x = data['carat'],y = data['price'])
plt.xlabel("Carat  ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Create scatter plot with 'depth' on X-axis and 'price' on Y-axis
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Scatter plot of Depth Vs Price Data Graph",fontsize = 20)
plt.scatter(x = data['depth'],y = data['price'])
plt.xlabel("Depth ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Create scatter plot with 'table' on X-axis and 'price' on Y-axis
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Scatter plot of Table Vs Price Data Graph",fontsize = 20)
plt.scatter(x = data['table'],y = data['price'])
plt.xlabel("Table ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Create scatter plot with attribute 'x' on X-axis and 'price' on Y-axis
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Scatter plot of X Vs Price Data Graph",fontsize = 20)
plt.scatter(x = data['x'],y = data['price'])
plt.xlabel("X ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Create scatter plot with attribute 'y' on X-axis and 'price' on Y-axis
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Scatter plot of Y Vs Price Data Graph",fontsize = 20)
plt.scatter(x = data['y'],y = data['price'])
plt.xlabel("Y ---- >")
plt.ylabel("Price ---- >")
plt.show()

# Create scatter plot with 'z' on X-axis and 'price' on Y-axis
plt.figure(figsize=(20,5),dpi = 96)
plt.title("Scatter plot of Z Vs Price Data Graph",fontsize = 20)
plt.scatter(x = data['z'],y = data['price'])
plt.xlabel("Z ---- >")
plt.ylabel("Price ---- >")
plt.show()

"""**Q:** Which attribute exhibit the best linear relationship with the target variable `price`?

**A:**
"""

# Create a normal distribution curve for the `price`.
from scipy.stats import norm
# Create a probablity density function for plotting the normal distribution
def prob_density_func(arr, mean, std):
    coeff = 1 / (std * np.sqrt(2*np.pi))
    power_of_e = np.exp(-(arr - mean) ** 2/ (2 * std ** 2))
    prob = coeff * power_of_e
    return prob

# Plot the normal distribution curve using plt.scatter()
plt.figure(figsize=(20,5),dpi = 96)
plt.scatter(x = data['price'], y = prob_density_func(data['price'], data['price'].mean(), data['price'].std()))
plt.title('Normal distribution curve for the price', fontsize = 15)
plt.xlabel('Price')
plt.axvline(data['price'].mean(), color = 'black', label=data['price'].mean())
plt.legend()
plt.show()
print(" Mean price :",data['price'].mean())

"""**Q:** What is the mean `price` of diamonds ?

**A:** Mean price of diamonds is :  3932.799721913237.

---

#### 3. Feature Engineering

The dataset contains certain features that are categorical.  To convert these features into numerical ones, use `replace()` function of the DataFrame.

**For example:**

`df["column1"].replace({"a": 1, "b": 0}, inplace=True)` $\Rightarrow$ replaces all the `'a'` values with `1` and `'b'` values with `0` for feature `column1`. Use `inplace` boolean argument to to make changes in the DataFrame permanently.

Replace following values for `cut` column:

 - `Fair` with `1`
 - `Good` with `2`
 - `Very Good` with `3`
 - `Premium` with `4`
 - `Ideal` with `5`

Replace following values for the `color` column:

- `D` with `1`
- `E` with `2`
- `F` with `3`
- `G` with `4`
- `H` with `5`
- `I` with `6`

Replace following values for the `clarity` column:

- `I1` with `1`
- `SI2` with `2`
- `SI1` with `3`
- `VS2` with `4`
- `VS1` with `5`
- `VVS2` with `6`
- `VVS1` with `7`
- `IF` with `8`
"""

# Replace values of 'cut' column
data['cut'].replace({'Fair' : 1,
                   'Good' : 2,
                   'Very Good' : 3,
                   'Premium' : 4,
                   'Ideal' : 5}, inplace= True)

# Replace values of 'color' column
data['color'].replace({'D' : 1,
                     'E' : 2,
                     'F' : 3,
                     'G' : 4,
                     'H' : 5,
                     'I' : 6,
                     'J' : 7}, inplace= True)

# Replace values of 'clarity' column
data['clarity'].replace({'I1' : 1,
                       'SI2' : 2,
                       'SI1' : 3,
                       'VS2' : 4,
                       'VS1' : 5,
                       'VVS2' : 6,
                       'VVS1' : 7,
                       'IF' : 8}, inplace = True)

"""---

#### 4. Model Training

Build a multiple linear regression model  using all the features of the dataset. Also, evaluate the model by calculating $R^2$, MSE, RMSE, and MAE values.
"""

# Create a list of feature variables.
features = list(data.columns)
features.remove('price')
print(features)

# Build multiple linear regression model using all the features

from sklearn.model_selection import train_test_split

# Split the DataFrame into the train and test sets such that test set has 33% of the values.
X = data[features]
y = data['price']

# Build linear regression model using the 'sklearn.linear_model' module.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.linear_model import LinearRegression

sklearn_lin_reg = LinearRegression()

y_train_reshaped = y_train.values.reshape(-1,1)
y_test_reshaped = y_test.values.reshape(-1,1)

sklearn_lin_reg.fit(X_train, y_train_reshaped)

# Print the value of the intercept
print("\nConstant".ljust(15, " "), f"{sklearn_lin_reg.intercept_[0]:.6f}")

# Print the names of the features along with the values of their corresponding coefficients.
for item in list(zip(X.columns.values, sklearn_lin_reg.coef_[0])):
    print(f"{item[0]}".ljust(15, " "), f"{item[1]:.3f}")

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

y_train_pred = sklearn_lin_reg.predict(X_train)
y_test_pred = sklearn_lin_reg.predict(X_test)

print(f"Train Set\n{'-' * 40}")
print(f"R-squared: {r2_score(y_train_reshaped, y_train_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_train_reshaped, y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train_reshaped, y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train_reshaped, y_train_pred):.3f}")

print(f"\n\nTest Set\n{'-' * 40}")
print(f"R-squared: {r2_score(y_test_reshaped, y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test_reshaped, y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_reshaped, y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test_reshaped, y_test_pred):.3f}")

"""**Q:** What is the $R^2$ (R-squared) value for this model?

**A:** R-squared of this model is 0.907 .

---

### 5. Dealing with Multicollinearity

Create a heatmap among all variables to identify a set of features which are highly correlated with each other.
"""

# Heatmap to pinpoint the columns in the 'df' DataFrame exhibiting high correlation
plt.figure(figsize = (20, 5), dpi = 96)
sns.heatmap(data.corr(), annot = True)
plt.show()

"""**Q:** Which features are highly correlated with `price`?

**A:** Carat , x , y and z are higly correlated with price.

**Q:** Is there multicollinearity in the dataset?

**A:** Yes there is multicollinearity in the dataset.

Let's consider the feature `carat` as it is highly correlated with the target variable `price`. Perform the following tasks:
1. Drop the features which are highly correlated with `carat`.
2. Calculate VIF (Variance Inflation Factor) for the remaining features.
"""

# Drop features highly correlated with 'carat'
df = data.drop(columns=['x','y','z'], axis = 1)
df.head()

# Again build a linear regression model using the remaining features
X1 = df[['carat','cut','color','clarity','depth','table']]
y1 = df['price']

X_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1, test_size = 0.33, random_state = 42)

# Build linear regression model using the 'sklearn.linear_model' module.
lin_reg = LinearRegression()

y_train2_reshaped = y_train2.values.reshape(-1,1)
y_test2_reshaped = y_test2.values.reshape(-1,1)

lin_reg.fit(X_train2, y_train_reshaped)

# Print the value of the intercept
print('Intercept'.ljust(15, ' '), f'{lin_reg.intercept_[0]:.6f}')


# Print the names of the features along with the values of their corresponding coefficients.
for item in (list(zip(X1.columns.values, lin_reg.coef_[0]))) :
    print(f'{item[0]}'.ljust(15,' '), f'{item[1] : .3f}')

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

y_train_pred = sklearn_lin_reg.predict(X_train)
y_test_pred = sklearn_lin_reg.predict(X_test)

print(f"Train Set\n{'-' * 40}")
print(f"R-squared: {r2_score(y_train_reshaped, y_train_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_train_reshaped, y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train_reshaped, y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train_reshaped, y_train_pred):.3f}")

print(f"\n\nTest Set\n{'-' * 40}")
print(f"R-squared: {r2_score(y_test_reshaped, y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test_reshaped, y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_reshaped, y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test_reshaped, y_test_pred):.3f}")

"""Now eliminate the features having VIF values above 10 (if any)."""

# Calculate the VIF values for the remaining features using the 'variance_inflation_factor' function.
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Add a constant to feature variables
X_train2_sm = sm.add_constant(X_train2)

# Create a dataframe that will contain the names of the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train2_sm.columns
vif['VIF'] = [variance_inflation_factor(X_train2_sm.values, i) for i in range(X_train2_sm.values.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by='VIF', ascending=False)
vif

"""**Q**: Which of the features have VIF values above 10?

**A**: No features have VIF values above 10.

Proceed with the below 4 code cells only if any of the features have VIF value above 10, otherwise jump to **6. Residual (Error) Analysis** section.
"""

# Create a list of features having VIF values less than 10

# Again build a linear regression model using the features whose VIF values are less than 10


# Build linear regression model using the 'sklearn.linear_model' module.


# Print the value of the intercept


# Print the names of the features along with the values of their corresponding coefficients.

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.

# Again calculate the VIF values for the remaining features to find out if there is still multicollinearity

"""---

#### 6. Residual (Error) Analysis

Perform residual analysis to check if the residuals (errors) are normally distributed or not (which is one of the assumption of linear regression). For this, plot the  histogram of the residuals.
"""

# Create a histogram for the errors obtained in the predicted values for the train set.
errors_train = y_train_reshaped - y_train_pred
plt.figure(figsize=(20,5),dpi = 96)
plt.hist(errors_train, bins='sturges', edgecolor='black')
plt.title('Histogram for the errors in train set', fontsize = 15)
plt.axvline(errors_train.mean(), label = f'Mean of the errors = {errors_train.mean():.3f}',color ='red')
plt.xlabel('Train set errors',fontsize = 15)
plt.legend()
plt.show()

# Create a histogram for the errors obtained in the predicted values for the test set.
errors_test = y_test_reshaped - y_test_pred
plt.figure(figsize=(20,5),dpi = 96)
plt.hist(errors_test, bins='sturges', edgecolor='black')
plt.title('Histogram for the errors in test set', fontsize=15)
plt.axvline(errors_test.mean(), label=f'Mean of the errors = {errors_test.mean() :.3f}',color ='red')
plt.xlabel('Test set errors  ----- >',fontsize = 15)
plt.legend()
plt.show()

"""**Q:** Is the mean of errors equal to 0 for train set?

**A:** Yes

**Q:** Is the mean of errors equal to 0 for test set?

**A:** No

---

#### 7. Verify Homoscedasticity

Check for Homoscedasticity (constant variance) by creating a scatter plot between the errors and the target variable. Determine whether there is some kind of relationship between the error and the target variable.
"""

# Create a scatter plot between the errors and the dependent variable for the train set.
plt.figure(figsize = (20,5),dpi = 96)
plt.title('Scatter plot between the errors and the dependent variable for the train set')
plt.scatter(x  = y_train, y = errors_train)
plt.axhline(y = errors_train.mean(), label = f'Mean of the errors = {errors_train.mean():.3f}',color ='red')
plt.legend()
plt.show()

"""**Q:** Do you find any pattern or trend in the scatter plot? Whether the residuals exhibit constant variance around mean of errors?

**A:** Yes , there is a pattern in the scatter plot.The variance in the errors is not constant instead it is increasing as we move from left-to-right. Hence there is heteroscedasticity between the errors w.r.t the predicted values.

---

### Submitting the Project

Follow the steps described below to submit the project.

1. After finishing the project, click on the **Share** button on the top right corner of the notebook. A new dialog box will appear.

  <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/2_share_button.png' width=500>

2. In the dialog box, click on the **Copy link** button.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/3_copy_link.png' width=500>


3. The link of the duplicate copy (named as **YYYY-MM-DD_StudentName_CapstoneProject16**) of the notebook will get copied

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/4_copy_link_confirmation.png' width=500>

4. Go to your dashboard and click on the **My Projects** option.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/5_student_dashboard.png' width=800>

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/6_my_projects.png' width=800>

5. Click on the **View Project** button for the project you want to submit.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/7_view_project.png' width=800>

6. Click on the **Submit Project Here** button.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/8_submit_project.png' width=800>

7. Paste the link to the project file named as **YYYY-MM-DD_StudentName_CapstoneProject16** in the URL box and then click on the **Submit** button.

   <img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/project-share-images/9_enter_project_url.png' width=800>

---
"""